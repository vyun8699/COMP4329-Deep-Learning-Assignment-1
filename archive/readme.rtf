{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fnil\fcharset0 Menlo-Regular;
}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red253\green183\blue24;\red193\green193\blue193;
\red0\green0\blue0;\red141\green213\blue254;\red193\green193\blue193;\red203\green203\blue202;\red194\green125\blue100;
\red71\green138\blue206;\red212\green213\blue153;\red167\green197\blue151;\red0\green0\blue0;\red255\green255\blue255;
\red193\green193\blue193;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;\cssrgb\c99711\c76105\c11148;\cssrgb\c80000\c80000\c80000;
\cssrgb\c0\c0\c0;\cssrgb\c61545\c86704\c99884;\cssrgb\c80176\c80176\c79976;\cssrgb\c83320\c83320\c83112;\cssrgb\c80772\c56796\c46790;
\cssrgb\c34146\c61677\c84338;\cssrgb\c86247\c86215\c66392;\cssrgb\c71035\c80830\c65726;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985\c0;
\cssrgb\c80088\c80088\c79988;\cssrgb\c100000\c100000\c100000\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww19900\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 COMP5329 - Deep Learning - Assignment 1\
Team : Andrew Zhang: (SID 500543568), Vincent Yunansan (SID 530454874)
\f1\b0 \
\
\ul A. BACKGROUND:\
\ulnone Goal of this assignment is to implement a neural network without the use of modern machine learning libraries. Teams are provided with training and test data, each with 50,000 and 10,000 data points across 10 classes. Each data point consists of 128 floating numbers.\ul \ulc0 \
\
B. CODE OVERVIEW:\ulnone \

\f0\b The codes can be ran top to bottom
\f1\b0 . The structure of our neural network implementation can be divided into three main parts, explained below:\
	\
\ul B1. PRE-PROCESSING:\
\ulnone \
1. 
\f0\b Normalization
\f1\b0 : training and testing data are normalised (mean = c.0, standard deviation = c.1). \
\
2. 
\f0\b resize_and_shuffle
\f1\b0 : (i) shuffle: the dataset provided was sorted by class, shuffling the data is important especially if we are using mini-batch. (ii) resize: we implemented the capability to downsize the dataset, so we do not have to run our training and testing with the full dataset during the building phase.\
	\
\ul Sample command: \ulnone \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cb3  
\f2 \expnd0\expndtw0\kerning0
data, label = resize_and_shuffle(input_data, input_label, proportion = 0.5, random_state = 42)\

\f1 \kerning1\expnd0\expndtw0 Explanation: this command resizes the training data by 0.5 x 50,000 = 25,000 with a set seed for replicability.\cf0 \cb1 \
 \
3. 
\f0\b train_val_split
\f1\b0 : splits the data into training and validation sets, especially important if we were to implement early stopping.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul Sample command:\ulnone  \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
train_data, train_label, val_data, val_label = train_val_split(input, output, proportion = 0.8, random_state = 42)\

\f1 \kerning1\expnd0\expndtw0 Explanation: this command splits the training data to 0.8 x 25,000 = 20,000 in the training data and 0.2 x 25,000 = 5,000 in the validation data with a set seed for replicability.\cf0 \cb1 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul B2. THE DEEP LEARNING MODEL:\ulnone \
The deep learning model and its methods are implemented in three classes:\
\
1. 
\f0\b Activation class
\f1\b0 : this object contains our activation functions and their derivatives. This class is called by HiddenLayer when the neural network is constructed. \
- Activations for hidden layers include tanh, sigmoid, relu, leaky relu\
- Activation for output layer is softmax.\
\
2. 
\f0\b HiddenLayer class
\f1\b0 : this object contains all parameters and methods relevant to the hidden layer. In summary, this class contains:\
- Weights, biases, and their gradients\
- forward and backward passes\
- Implementation of momentum, weight decay, dropout, batch normalisation, and Adam.\
\
3. 
\f0\b MLP
\f1\b0  
\f0\b class
\f1\b0 : this object contains the main constructor and iterator for our neural network. In summary, this class contains:\
- initial neural network constructor\
- forward and backward: which will trigger the forward and backward propagations in all layers\
- criterion cross entropy: which is our objective function\
- update: which will update weights and gradients after forward and backward propagation\
- getBatch: which functions as a data loader for mini batch\
- fit: which will fit our model with the training data, do periodical validation checks, trigger early stopping (if used), \
- predict: which predicts labels based on the trained neural network\
- eval: which evaluates model performance during training\
- train_loop: which implements a single forward, backward, and update loop, to be used in fit.\
\
\ul sample commands:\ulnone \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
nn = MLP([128,64,64,10], [None, \'91relu\'92,\'92relu\'92,\'92softmax\'92])\

\f1 \kerning1\expnd0\expndtw0 Explanation: This constructs a neural network object with two hidden layers (relu, 64 nodes each) and one output layer (softmax, 10 nodes). \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \cb1 	\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
train_loss, val_loss, epoch = nn.fit(train_data, train_label, val_data, val_label, \
\pard\pardeftab720\partightenfactor0
\cf2                                      momentum_gamma = 0.0, learning_rate = 0.001, epochs = 200, batch_size = 100, \
                                     weight_decay = 0.0, dropout_rate = 0.0, early_stopping = [3,10],  \
                                     batchnorm_switch = False, adam_switch = False, adam_learning_rate = 0.0)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf2 \kerning1\expnd0\expndtw0 Explanation: this implements a fit with training and validation data and labels, with different parameters (e.g. learning_rate, epochs, batch_size) and methods. Further explanation is provided in comments within the code file\cf0 \cb1 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul B3. HELPER FUNCTIONS:\ulnone \
1. 
\f0\b Hyperparameter_testing
\f1\b0 : \
	- a hyperparameter tuning function is implemented to simplify the repeated hyperparameter testing\
	- this function takes all possible combination of parameters as lists and iterates through them\
	- this function also calculates training and testing scores, records the number of epochs for each training run, and runtimes\
\
\ul sample command:\ulnone \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
learning_rates = [0.001]\
\pard\pardeftab720\partightenfactor0
\cf2 batch_sizes = [1]\
early_stopping_combination = [[np.inf,10],[3,10]]\
epoch_counts = [200]\
momentum_gammas = [0.0]\
weight_decays = [0.0]\
dropouts = [0.0]\
batchnorm_switches = [False]\
adam_learning_rates = [0.00]\
node_counts = [[128,64,64,10]] \
node_activations = [[None,'relu','relu','softmax']]\
\
parameters = (learning_rates,batch_sizes,early_stopping_combination, epoch_counts, momentum_gammas, weight_decays, dropouts, batchnorm_switches,adam_learning_rates, node_counts,node_activations)\
\
results = hyperparamater_testing(parameters,train_data, train_label, val_data, val_label)\cf4 \cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf2 \cb3 \ul \ulc5 Explanation\ulnone : n the above example, hyperparameter tuning was performed over early_stopping combination \
e.g. ([np.inf,10] for no early stopping, [3,10] for early stopping with 3 max cycle of deteriorating validation score and validation checks every 10 epochs).\
this function returns relevant parameters and scores for reporting purposes and train and validation losses for plotting purposes.\
Further explanation is provided in comments within the code file\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \cb1 \
2. 
\f0\b filter_asset
\f1\b0 :\
- this function creates a data frame from hyperparameter tuning results, for reporting purposes\
- users can choose how to filter the dataset (e.g. only include results where learning_rate = 0.001) and which columns to show in the data frame.\
\
3. 
\f0\b multi_plotter
\f1\b0 :\
- This function plots training loss curves with customisable color, marker, and line.\
- list down all column names in label_keys to display them in the legend box.\
- add the name of the label in marker_param, line_param, and color_param to add them as customisation.	\
Sample command:\

\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
target ='train_loss'\
\pard\pardeftab720\partightenfactor0
\cf2 label_keys =['early_stopping']  \
marker_param = None\
line_param = 'early_stopping'\
color_param = 'early_stopping'\
title = 'Training Losses with and without early stopping'\
multi_plotter(title, assets_exp_1, target, label_keys, marker_param, line_param, color_param, fig_size =(10,3))
\f1 \cf13 \cb14 \
\pard\pardeftab720\partightenfactor0
\cf15 \cb16 \
}