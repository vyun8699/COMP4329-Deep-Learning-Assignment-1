{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#printer function\n",
    "def print_dim(name,input):\n",
    "    print(f'shape of {name}: {input.shape}')\n",
    "\n",
    "def print_arr(name,input):\n",
    "    print(f'array of {name}: {input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def lin_transform(input, W, b):\n",
    "    output = np.dot(input,W) + b\n",
    "    return output\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x>0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x- np.max(x, axis =1, keepdims = True))\n",
    "    return exp_x / np.sum(exp_x, axis =1, keepdims = True)\n",
    "\n",
    "def softmax_deriv(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def y_pred_allocation(x):\n",
    "    return np.argmax(x)\n",
    "\n",
    "def loss(y_true, softmax_output):\n",
    "    n_class = softmax_output.shape[1]\n",
    "    n_samples = softmax_output.shape[0]\n",
    "    y_true_ohe = np.zeros((n_samples, n_class))\n",
    "    y_true = y_true.flatten().astype(int)\n",
    "    y_true_ohe[np.arange(n_samples),y_true] = 1\n",
    "\n",
    "    return softmax_output-y_true_ohe\n",
    "\n",
    "def forward(input, hidden_W, hidden_b, output_W, output_b):\n",
    "    hidden_input = lin_transform(input, hidden_W, hidden_b)\n",
    "    relu_output = relu(hidden_input)\n",
    "    softmax_input = lin_transform(relu_output, output_W, output_b)\n",
    "    softmax_output = softmax(softmax_input)\n",
    "    print('FORWARD:')\n",
    "    print_dim('hidden_input', hidden_input)\n",
    "    print_dim('relu_output', relu_output)\n",
    "    print_dim('softmax_input', softmax_input)\n",
    "    print_dim('softmax_output', softmax_output)\n",
    "    print('')\n",
    "    #print_arr('softmax_input', softmax_input)\n",
    "    #print_arr('softmax_output', softmax_output)\n",
    "    #print('')\n",
    "\n",
    "    return softmax_output, relu_output\n",
    "\n",
    "def backward(input,y_true, softmax_output, hidden_output, hidden_W, hidden_b, output_W, output_b):\n",
    "    # loss in output layer\n",
    "    loss_output = loss(y_true, softmax_output)\n",
    "\n",
    "    # Gradients of the output layer\n",
    "    grad_output_W = np.dot(hidden_output.T, loss_output)\n",
    "    grad_output_b = np.sum(loss_output, axis=0)\n",
    "    \n",
    "    # loss in hidden layer\n",
    "    loss_hidden = np.dot(loss_output, output_W.T) * relu_deriv(hidden_output)\n",
    "\n",
    "    # Gradients of the hidden layer\n",
    "    grad_hidden_W = np.dot(input.T, loss_hidden)\n",
    "    grad_hidden_b = np.sum(loss_hidden, axis=0)\n",
    "\n",
    "    print('BACKWARD:')\n",
    "    print_dim('y_true', y_true)\n",
    "    print_dim('softmax_output', softmax_output)\n",
    "    print_dim('loss_output',loss_output)\n",
    "    print_dim('grad_output_W',grad_output_W)\n",
    "    print_dim('grad_output_b',grad_output_b)\n",
    "\n",
    "    print('')\n",
    "    print_dim('loss_hidden',loss_hidden)\n",
    "    print_dim('grad_hidden_W',grad_hidden_W)\n",
    "    print_dim('grad_hidden_b',grad_hidden_b)    \n",
    "    print('')\n",
    "    \n",
    "    return grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b\n",
    "\n",
    "def update(lr, hidden_W, hidden_b, output_W, output_b, grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b):\n",
    "    hidden_W -= lr * grad_hidden_W\n",
    "    hidden_b -= lr * grad_hidden_b\n",
    "    output_W -= lr * grad_output_W\n",
    "    output_b -= lr * grad_output_b\n",
    "    print('UPDATE:')\n",
    "    print_dim(\"hidden_W\", hidden_W)\n",
    "    print_dim(\"hidden_b\",hidden_b)\n",
    "    print_dim(\"output_W\",output_W)\n",
    "    print_dim(\"output_b\",output_b)\n",
    "    print('')\n",
    "\n",
    "    return hidden_W, hidden_b, output_W, output_b\n",
    "\n",
    "def predict(softmax_output):\n",
    "    y_pred = np.argmax(softmax_output, axis = 1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input: (1, 5)\n",
      "shape of hidden_W: (5, 3)\n",
      "shape of hidden_b: (3,)\n",
      "shape of output_W: (3, 2)\n",
      "shape of output_b: (2,)\n",
      "shape of y true: (1,)\n",
      "\n",
      "array of hidden_W: [[-0.29757411 -1.56526315  0.51642671]\n",
      " [-1.43666905  0.35057554 -0.63461515]\n",
      " [-0.31406078 -0.17363556 -0.69292359]\n",
      " [-2.54358023  0.81889583  0.58924507]\n",
      " [-0.4299372   1.18089037  0.26863935]]\n",
      "array of output_W: [[ 0.01071466  1.08207792]\n",
      " [-1.26716354 -0.64267912]\n",
      " [ 0.4719933  -1.50544545]]\n",
      "array of y_true: [1]\n"
     ]
    }
   ],
   "source": [
    "#initialize dataset\n",
    "count_data = 1\n",
    "n_class = 2\n",
    "n_data = 5 # length of each data\n",
    "n_hidden = 3 # width of hidden layer\n",
    "n_output = n_class # width of output layer\n",
    "\n",
    "# initialize each layer\n",
    "input = np.ones((count_data,n_data))\n",
    "hidden_W = np.random.randn(n_data,n_hidden)\n",
    "hidden_b = np.zeros(n_hidden,)\n",
    "output_W = np.random.randn(n_hidden,n_output)\n",
    "output_b = np.zeros(n_output)\n",
    "\n",
    "y_true = np.random.randint(0,n_class, count_data)\n",
    "\n",
    "print_dim(\"input\",input)\n",
    "print_dim(\"hidden_W\", hidden_W)\n",
    "print_dim(\"hidden_b\",hidden_b)\n",
    "print_dim(\"output_W\",output_W)\n",
    "print_dim(\"output_b\",output_b)\n",
    "print_dim('y true', y_true)\n",
    "print('')\n",
    "print_arr('hidden_W', hidden_W)\n",
    "print_arr('output_W', output_W)\n",
    "print_arr('y_true', y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD:\n",
      "shape of hidden_input: (1, 3)\n",
      "shape of relu_output: (1, 3)\n",
      "shape of softmax_input: (1, 2)\n",
      "shape of softmax_output: (1, 2)\n",
      "\n",
      "BACKWARD:\n",
      "shape of y_true: (1,)\n",
      "shape of softmax_output: (1, 2)\n",
      "shape of loss_output: (1, 2)\n",
      "shape of grad_output_W: (3, 2)\n",
      "shape of grad_output_b: (2,)\n",
      "\n",
      "shape of loss_hidden: (1, 3)\n",
      "shape of grad_hidden_W: (5, 3)\n",
      "shape of grad_hidden_b: (3,)\n",
      "\n",
      "UPDATE:\n",
      "shape of hidden_W: (5, 3)\n",
      "shape of hidden_b: (3,)\n",
      "shape of output_W: (3, 2)\n",
      "shape of output_b: (2,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#forward\n",
    "softmax_output, relu_output = forward(input,hidden_W, hidden_b, output_W, output_b)\n",
    "\n",
    "#backward\n",
    "grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b = backward(input,y_true, softmax_output, relu_output, \n",
    "                                                                      hidden_W, hidden_b, output_W, output_b)\n",
    "\n",
    "#implement update\n",
    "hidden_W, hidden_b, output_W, output = update(0.1, hidden_W, hidden_b, output_W, output_b, \n",
    "                                              grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(softmax_output)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin transform in hidden layer:\n",
      "shape of input: (1, 5)\n",
      "shape of hidden_W: (5, 3)\n",
      "shape of hidden_b: (3,)\n",
      "shape of lin_transform_hidden: (1, 3)\n",
      "\n",
      "relu output in hidden layer:\n",
      "shape of relu_output: (1, 3)\n",
      "\n",
      "lin transform in softmax layer:\n",
      "shape of input: (1, 3)\n",
      "shape of hidden_W: (3, 2)\n",
      "shape of hidden_b: (2,)\n",
      "shape of lin_transform_softmax: (1, 2)\n",
      "\n",
      "softmax calculation:\n",
      "shape of np_max_output: (1, 1)\n",
      "shape of relu_output: (1, 3)\n",
      "shape of softmax_numerator: (1, 2)\n",
      "shape of softmax_denominator: (1, 1)\n",
      "shape of softmax_output: (1, 2)\n",
      "\n",
      "softmax loss calculation:\n",
      "shape of y_true: (1,)\n",
      "shape of softmax_output: (1, 2)\n",
      "shape of softmax_loss: (1, 2)\n",
      "\n",
      "softmax grad calculation:\n",
      "shape of relu_output.T: (3, 1)\n",
      "shape of softmax_loss: (1, 2)\n",
      "shape of softmax_grad_W: (3, 2)\n",
      "shape of softmax_grad_b: (2,)\n",
      "\n",
      "shape of softmax_loss: (1, 2)\n",
      "shape of output_W.T: (2, 3)\n",
      "shape of relu_output: (1, 3)\n",
      "relu loss calculation:\n",
      "shape of relu_loss: (1, 3)\n",
      "\n",
      "relu grad calculation:\n",
      "shape of input.T: (5, 1)\n",
      "shape of relu_loss: (1, 3)\n",
      "shape of hidden_grad_W: (5, 3)\n",
      "shape of hidden_grad_b: (3,)\n",
      "\n",
      "update calculation:\n",
      "shape of hidden_W: (5, 3)\n",
      "shape of hidden_b: (3,)\n",
      "shape of output_W: (3, 2)\n",
      "shape of output_b: (2,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#step by step\n",
    "\n",
    "\n",
    "#FORWARD\n",
    "\n",
    "#hidden layer\n",
    "lin_transform_hidden = np.dot(input,hidden_W) + hidden_b \n",
    "\n",
    "print('lin transform in hidden layer:')\n",
    "print_dim('input', input)\n",
    "print_dim('hidden_W', hidden_W)\n",
    "print_dim('hidden_b', hidden_b)\n",
    "print_dim('lin_transform_hidden', lin_transform_hidden)\n",
    "print('')\n",
    "\n",
    "relu_output = relu(lin_transform_hidden) #relu activation\n",
    "print('relu output in hidden layer:')\n",
    "print_dim('relu_output', relu_output)\n",
    "print('')\n",
    "\n",
    "#softmax\n",
    "lin_transform_softmax = np.dot(relu_output,output_W) + output_b # lin transform at softmax\n",
    "print('lin transform in softmax layer:')\n",
    "print_dim('input', relu_output)\n",
    "print_dim('hidden_W', output_W)\n",
    "print_dim('hidden_b', output_b)\n",
    "print_dim('lin_transform_softmax', lin_transform_softmax)\n",
    "print('')\n",
    "\n",
    "np_max_output = np.max(lin_transform_softmax, axis = -1 , keepdims = True) \n",
    "softmax_numerator = np.exp(lin_transform_softmax - np_max_output)\n",
    "softmax_denominator = np.sum(np_max_output, axis = -1, keepdims = True)\n",
    "softmax_output = softmax_numerator / softmax_denominator\n",
    "print('softmax calculation:')\n",
    "print_dim('np_max_output', np_max_output)\n",
    "print_dim('relu_output', relu_output)\n",
    "print_dim('softmax_numerator', softmax_numerator)\n",
    "print_dim('softmax_denominator', softmax_denominator)\n",
    "print_dim('softmax_output', softmax_output)\n",
    "print('')\n",
    "\n",
    "#BACKWARD\n",
    "\n",
    "# softmax loss\n",
    "softmax_loss = loss(y_true,softmax_output) \n",
    "print('softmax loss calculation:')\n",
    "print_dim('y_true', y_true)\n",
    "print_dim('softmax_output', softmax_output)\n",
    "print_dim('softmax_loss', softmax_loss)\n",
    "print('')\n",
    "\n",
    "#softmax grad calculation\n",
    "softmax_grad_W = np.dot(relu_output.T, softmax_loss) #output_grad_W\n",
    "softmax_grad_b = np.sum(softmax_loss, axis =0) #output_grad_b\n",
    "print('softmax grad calculation:')\n",
    "print_dim('relu_output.T', relu_output.T)\n",
    "print_dim('softmax_loss', softmax_loss)\n",
    "print_dim('softmax_grad_W', softmax_grad_W)\n",
    "print_dim('softmax_grad_b', softmax_grad_b)\n",
    "print('')\n",
    "\n",
    "#relu loss\n",
    "\n",
    "relu_loss = np.dot(softmax_loss, output_W.T) * relu_deriv(relu_output)# loss relu\n",
    "print_dim('softmax_loss', softmax_loss)\n",
    "print_dim('output_W.T', output_W.T)\n",
    "print_dim('relu_output', relu_output)\n",
    "print('relu loss calculation:')\n",
    "print_dim('relu_loss', relu_loss)\n",
    "print('')\n",
    "\n",
    "# relu grad calculation\n",
    "hidden_grad_W = np.dot(input.T, relu_loss) #hidden_grad_W\n",
    "hidden_grad_b = np.sum(relu_loss, axis =0) #hidden_grad_b\n",
    "print('relu grad calculation:')\n",
    "print_dim('input.T', input.T)\n",
    "print_dim('relu_loss', relu_loss)\n",
    "print_dim('hidden_grad_W', hidden_grad_W)\n",
    "print_dim('hidden_grad_b', hidden_grad_b)\n",
    "print('')\n",
    "\n",
    "#update calculation\n",
    "lr = 0.1\n",
    "hidden_W -= lr * grad_hidden_W\n",
    "hidden_b -= lr * grad_hidden_b\n",
    "output_W -= lr * grad_output_W\n",
    "output_b -= lr * grad_output_b\n",
    "print('update calculation:')\n",
    "print_dim('hidden_W', hidden_W)\n",
    "print_dim('hidden_b', hidden_b)\n",
    "print_dim('output_W', output_W)\n",
    "print_dim('output_b', output_b)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BN implementation\n",
    "\n",
    "def bn_forward(x, gamma, beta, eps):\n",
    "    N, D = x.shape\n",
    "    mean = 1./N * np.sum(input, axis = 0)\n",
    "    input_mean = input - mean\n",
    "    var = 1./N * np.sum(input_mean ** 2, axis = 0)\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "    ivar = 1./sqrtvar\n",
    "    input_hat = input_mean * ivar\n",
    "    gammax = gamma * input_hat\n",
    "    output = gammax + beta\n",
    "    cache = (input_hat, gamma, input_mean, ivar, sqrtvar, var, eps)\n",
    "    print('BN FORWARD')\n",
    "    print_dim('input', input)\n",
    "    print_dim('gamma', gamma)\n",
    "    print_dim('beta', beta)\n",
    "    print_dim('mean', mean)\n",
    "    print_dim('input_mean', input_mean)\n",
    "    print_dim('var', var)\n",
    "    print_dim('sqrtvar', sqrtvar)\n",
    "    print_dim('ivar', ivar)\n",
    "    print_dim('input_hat', input_hat)\n",
    "    print_dim('gammax', gammax)\n",
    "    print_dim('output', output)\n",
    "    return output, cache\n",
    "\n",
    "def bn_backward(dout, cache):\n",
    "    x_hat, gamma, x_mean, ivar, sqrtvar, var, eps = cache\n",
    "    N,D = dout.shape\n",
    "\n",
    "    dbeta = np.sum(dout, axis = 0)\n",
    "    dgammax = dout\n",
    "    dgamma = np.sum(dgammax*x_hat, axis = 0)\n",
    "    dxhat = dgammax * gamma\n",
    "    divar = np.sum(dxhat * x_mean, axis = 0)\n",
    "    dxmu1 = dxhat * ivar\n",
    "    dsqrtvar = -1. / (sqrtvar**2) * divar\n",
    "    dvar = 0.5 * 1./np.sqrt(var+eps) * dsqrtvar\n",
    "    dsq = 1./N * np.ones((N,D))*dvar\n",
    "    dxmu2 = 2*x_mean * dsq\n",
    "    dx1 = (dxmu1 + dxmu2)\n",
    "    dmu = -1 * np.sum(dxmu1+dxmu2, axis = 0)\n",
    "    dx2 = 1./N * np.ones((N,D)) * dmu\n",
    "    dx = dx1 + dx2\n",
    "\n",
    "    print('BN BACKWARD')\n",
    "    print_dim('dbeta', input)\n",
    "    print_dim('dgammax', dgammax)\n",
    "    print_dim('xhat', x_hat)\n",
    "    print_dim('dgamma', dgamma)\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]\n",
      " [8.]]\n",
      "[[2.]\n",
      " [2.]]\n",
      "[[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      " [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "matrix_mean = np.mean(matrix, axis=1, keepdims=True)\n",
    "matrix_var = np.var(matrix, axis=1, keepdims=True)\n",
    "matrix_subtracted = matrix - matrix_mean\n",
    "matrix_std = np.sqrt(matrix_var)\n",
    "matrix_hat = matrix_subtracted / matrix_std\n",
    "\n",
    "gamma_softmax = np.ones((matrix_hat.shape))\n",
    "beta_softmax = np.zeros(matrix.shape[-1])\n",
    "output = matrix_hat * gamma_softmax + beta_softmax\n",
    "\n",
    "print(matrix_mean)\n",
    "print(matrix_var)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "x_hat\n",
      "[[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      " [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]]\n",
      "[[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      " [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]]\n",
      "dgamma\n",
      "[4. 1. 0. 1. 4.]\n",
      "dxhat\n",
      "[[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n",
      " [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "#FORWARD\n",
    "N,D = matrix.shape\n",
    "\n",
    "gamma = np.ones((x.shape))\n",
    "print('gamma')\n",
    "print(gamma)\n",
    "beta = np.zeros(x.shape[-1])\n",
    "eps = 1e-9\n",
    "\n",
    "mean = 1./D * np.sum(x, axis = 1, keepdims = True)\n",
    "x_mean = x - mean\n",
    "var = 1./D * np.sum(x_mean ** 2, axis = 1, keepdims = True)\n",
    "sqrtvar = np.sqrt(var + eps)\n",
    "ivar = 1./sqrtvar\n",
    "x_hat = x_mean * ivar\n",
    "print('x_hat')\n",
    "print(x_hat)\n",
    "gammax = gamma * x_hat\n",
    "output = gammax + beta\n",
    "print(output)\n",
    "\n",
    "\n",
    "\n",
    "cache = (x_hat, gamma, x_mean, ivar, sqrtvar, var, eps)\n",
    "#BACKWARDS\n",
    "x_hat, gamma, x_mean, ivar, sqrtvar, var, eps = cache\n",
    "\n",
    "N,D = output.shape\n",
    "\n",
    "dbeta = np.sum(output, axis = 1)\n",
    "dgammax = output\n",
    "\n",
    "dgamma = np.sum(dgammax*x_hat, axis = 0)\n",
    "print('dgamma')\n",
    "print(dgamma)\n",
    "\n",
    "dxhat = dgammax * gamma\n",
    "print('dxhat')\n",
    "print(dxhat)\n",
    "divar = np.sum(dxhat * x_mean, axis = 0)\n",
    "dxmu1 = dxhat * ivar\n",
    "dsqrtvar = -1. / (sqrtvar**2) * divar\n",
    "dvar = 0.5 * 1./np.sqrt(var+eps) * dsqrtvar\n",
    "dsq = 1./N * np.ones((N,D))*dvar\n",
    "dxmu2 = 2*x_mean * dsq\n",
    "dx1 = (dxmu1 + dxmu2)\n",
    "dmu = -1 * np.sum(dxmu1+dxmu2, axis = 0)\n",
    "dx2 = 1./N * np.ones((N,D)) * dmu\n",
    "dx = dx1 + dx2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of lin_input: (3, 2)\n",
      "shape of mean: (2,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (3,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[586], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m ivar \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39msqrtvar\n\u001b[1;32m     15\u001b[0m input_hat \u001b[38;5;241m=\u001b[39m input_mean \u001b[38;5;241m*\u001b[39m ivar\n\u001b[0;32m---> 16\u001b[0m gammax \u001b[38;5;241m=\u001b[39m \u001b[43mgamma_softmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_hat\u001b[49m\n\u001b[1;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m gammax \u001b[38;5;241m+\u001b[39m beta_softmax\n\u001b[1;32m     18\u001b[0m cache \u001b[38;5;241m=\u001b[39m (input_hat, gamma_softmax, input_mean, ivar, sqrtvar, var, eps)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (3,2) "
     ]
    }
   ],
   "source": [
    "\n",
    "eps = 1e-9\n",
    "lin_input = lin_transform(relu_output, output_W, output_b)\n",
    "print_dim('lin_input', lin_input)\n",
    "\n",
    "gamma_softmax = np.ones((lin_input.shape))\n",
    "beta_softmax = np.zeros((lin_input.shape[-1]))\n",
    "print_dim('gamma', gamma_softmax)\n",
    "print_dim('beta', beta_softmax)\n",
    "\n",
    "matrix_mean = np.mean(matrix, axis=1, keepdims=True)\n",
    "matrix_var = np.var(matrix, axis=1, keepdims=True)\n",
    "matrix_subtracted = matrix - matrix_mean\n",
    "matrix_std = np.sqrt(matrix_var)\n",
    "matrix_hat = matrix_subtracted / matrix_std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN FORWARD\n",
      "shape of input: (1, 3)\n",
      "shape of gamma: (3, 3)\n",
      "shape of beta: (3,)\n",
      "shape of mean: (1,)\n",
      "shape of input_mean: (1, 3)\n",
      "shape of var: (1,)\n",
      "shape of sqrtvar: (1,)\n",
      "shape of ivar: (1,)\n",
      "shape of input_hat: (1, 3)\n",
      "shape of gammax: (3, 3)\n",
      "shape of output: (3, 3)\n",
      "FORWARD RELU:\n",
      "shape of input: (1, 5)\n",
      "shape of hidden_W: (5, 3)\n",
      "shape of hidden_b: (3,)\n",
      "shape of lin_input: (1, 3)\n",
      "shape of hidden_input: (3, 3)\n",
      "shape of gamma_relu: (3, 3)\n",
      "shape of beta_relu: (3,)\n",
      "shape of relu_output: (3, 3)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[661], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m softmax_output, cache\n\u001b[0;32m---> 48\u001b[0m softmax_output, softmax_cache \u001b[38;5;241m=\u001b[39m \u001b[43mforward_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelu_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_softmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_softmax\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[661], line 32\u001b[0m, in \u001b[0;36mforward_softmax\u001b[0;34m(input, output_W, output_b, gamma_softmax, beta_softmax)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_softmax\u001b[39m(\u001b[38;5;28minput\u001b[39m, output_W, output_b, gamma_softmax, beta_softmax):\n\u001b[1;32m     31\u001b[0m     lin_input \u001b[38;5;241m=\u001b[39m lin_transform(\u001b[38;5;28minput\u001b[39m, output_W, output_b)\n\u001b[0;32m---> 32\u001b[0m     softmax_input, cache \u001b[38;5;241m=\u001b[39m \u001b[43mbn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_softmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_softmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#BN INSERT\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     softmax_output \u001b[38;5;241m=\u001b[39m softmax(softmax_input)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFORWARD SOFTMAX:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[579], line 6\u001b[0m, in \u001b[0;36mbn_forward\u001b[0;34m(input, gamma, beta, eps)\u001b[0m\n\u001b[1;32m      4\u001b[0m N, D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      5\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mN \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28minput\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m input_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\n\u001b[1;32m      7\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mN \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(input_mean \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m sqrtvar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(var \u001b[38;5;241m+\u001b[39m eps)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (3,) "
     ]
    }
   ],
   "source": [
    "#forward\n",
    "\n",
    "gamma_relu = np.ones((hidden_W.shape[-1],hidden_W.shape[-1]))\n",
    "beta_relu = np.zeros((hidden_b.shape))\n",
    "eps = 1e-9\n",
    "\n",
    "#implementation of BN forward on relu layer\n",
    "def forward_relu(input, hidden_W, hidden_b, gamma_relu, beta_relu):\n",
    "    lin_input = lin_transform(input, hidden_W, hidden_b)\n",
    "    relu_input, cache = bn_forward(lin_input, gamma_relu, beta_relu, eps) #BN INSERT\n",
    "    relu_output = relu(relu_input)\n",
    "    print('FORWARD RELU:')\n",
    "    print_dim('input', input)\n",
    "    print_dim('hidden_W', hidden_W)\n",
    "    print_dim('hidden_b', hidden_b)\n",
    "    print_dim('lin_input', lin_input)\n",
    "    print_dim('hidden_input', relu_input)\n",
    "    print_dim('gamma_relu', gamma_relu)\n",
    "    print_dim('beta_relu', beta_relu)\n",
    "    print_dim('relu_output', relu_output)\n",
    "    print('')\n",
    "\n",
    "    return relu_output, cache\n",
    "\n",
    "relu_output, relu_cache = forward_relu(input,hidden_W, hidden_b, gamma_relu, beta_relu)\n",
    "\n",
    "gamma_softmax = np.ones((output_W.shape[-1],output_W.shape[-1]))\n",
    "beta_softmax = np.zeros((output_b.shape))\n",
    "\n",
    "def forward_softmax(input, output_W, output_b, gamma_softmax, beta_softmax):\n",
    "    lin_input = lin_transform(input, output_W, output_b)\n",
    "    softmax_input, cache = bn_forward(lin_input, gamma_softmax, beta_softmax, eps) #BN INSERT\n",
    "    softmax_output = softmax(softmax_input)\n",
    "    print('FORWARD SOFTMAX:')\n",
    "    print_dim('input', input)\n",
    "    print_dim('output_W', output_W)\n",
    "    print_dim('output_b', output_b)\n",
    "    print_dim('lin_input', lin_input)\n",
    "    print_dim('softmax_input', softmax_input)\n",
    "    print_dim('gamma_softmax', gamma_softmax)\n",
    "    print_dim('beta_softmax', beta_softmax)\n",
    "    print_dim('softmax_input', softmax_input)\n",
    "    print_dim('softmax_output', softmax_output)\n",
    "    print('')\n",
    "\n",
    "    return softmax_output, cache\n",
    "\n",
    "softmax_output, softmax_cache = forward_softmax(relu_output, output_W, output_b, gamma_softmax, beta_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN BACKWARD\n",
      "shape of dbeta: (10, 5)\n",
      "shape of dgammax: (10, 3)\n",
      "shape of xhat: (10, 3)\n",
      "shape of dgamma: (3,)\n",
      "BACKWARD:\n",
      "shape of y_true: (10,)\n",
      "shape of softmax_output: (10, 2)\n",
      "shape of loss_output: (10, 2)\n",
      "shape of grad_output_W: (3, 2)\n",
      "shape of grad_output_b: (2,)\n",
      "shape of dx: (10, 3)\n",
      "shape of dgamma: (3,)\n",
      "shape of dbeta: (3,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#backward\n",
    "\n",
    "def backward_softmax(y_true, softmax_output, hidden_output, output_W, output_b, softmax_cache):\n",
    "    # loss in output layer\n",
    "    loss_output = loss(y_true, softmax_output)\n",
    "\n",
    "    # Gradients of the output layer\n",
    "    grad_output_W = np.dot(hidden_output.T, loss_output)\n",
    "    grad_output_b = np.sum(loss_output, axis=0)\n",
    "    \n",
    "    #propagate through BN layer\n",
    "    bn_input = np.dot(loss_output, output_W.T)\n",
    "    dx, dgamma, dbeta = bn_backward(bn_input, softmax_cache)\n",
    "\n",
    "    print('BACKWARD:')\n",
    "    print_dim('y_true', y_true)\n",
    "    print_dim('softmax_output', softmax_output)\n",
    "    print_dim('loss_output',loss_output)\n",
    "    print_dim('grad_output_W',grad_output_W)\n",
    "    print_dim('grad_output_b',grad_output_b)\n",
    "    print_dim('dx',dx)\n",
    "    print_dim('dgamma',dgamma)\n",
    "    print_dim('dbeta',dbeta)\n",
    "    print('')\n",
    "\n",
    "    return  dx, grad_output_W, grad_output_b, dgamma, dbeta\n",
    "\n",
    "dx, grad_output_W, grad_output_b, dgamma, dbeta = backward_softmax(y_true, softmax_output, relu_output, output_W, output_b, softmax_cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
