{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printer function\n",
    "def print_dim(name,input):\n",
    "    print(f'shape of {name}: {input.shape}')\n",
    "\n",
    "def print_arr(name,input):\n",
    "    print(f'array of {name}: {input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def lin_transform(input, W, b):\n",
    "    output = np.dot(input,W) + b\n",
    "    return output\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x>0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x- np.max(x, axis =1, keepdims = True))\n",
    "    return exp_x / np.sum(exp_x, axis =1, keepdims = True)\n",
    "\n",
    "def softmax_deriv(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def y_pred_allocation(x):\n",
    "    return np.argmax(x)\n",
    "\n",
    "def forward(input, hidden_W, hidden_b, output_W, output_b):\n",
    "    hidden_input = lin_transform(input, hidden_W, hidden_b)\n",
    "    relu_output = relu(hidden_input)\n",
    "    softmax_input = lin_transform(relu_output, output_W, output_b)\n",
    "    softmax_output = softmax(softmax_input)\n",
    "    print_dim('hidden_input', hidden_input)\n",
    "    print_dim('relu_output', relu_output)\n",
    "    print_dim('softmax_input', softmax_input)\n",
    "    print_dim('softmax_output', softmax_output)\n",
    "    print('')\n",
    "    print_arr('softmax_input', softmax_input)\n",
    "    print_arr('softmax_output', softmax_output)\n",
    "\n",
    "    return softmax_output, relu_output\n",
    "\n",
    "def backward(input,y_true, y_pred, hidden_output, hidden_W, hidden_b, output_W, output_b):\n",
    "    # loss in output layer\n",
    "    loss_output = softmax_deriv(y_true, y_pred)\n",
    "\n",
    "    # Gradients of the output layer\n",
    "    grad_output_W = np.dot(hidden_output.T, loss_output)\n",
    "    grad_output_b = np.sum(loss_output, axis=0)\n",
    "    \n",
    "    # loss in hidden layer\n",
    "    loss_hidden = np.dot(loss_output, output_W.T) * relu_deriv(hidden_output)\n",
    "\n",
    "    # Gradients of the hidden layer\n",
    "    grad_hidden_W = np.dot(input.T, loss_hidden)\n",
    "    grad_hidden_b = np.sum(loss_hidden, axis=0)\n",
    "\n",
    "    print_dim('loss_output',loss_output)\n",
    "    print_dim('grad_hidden_W',grad_hidden_W)\n",
    "    print_dim('grad_hidden_b',grad_hidden_b)\n",
    "    print('')\n",
    "    print_dim('loss_hidden',loss_hidden)\n",
    "    print_dim('grad_output_W',grad_output_W)\n",
    "    print_dim('grad_output_b',grad_output_b)\n",
    "    \n",
    "    return grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b\n",
    "\n",
    "def update(lr, hidden_W, hidden_b, output_W, output_b, grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b):\n",
    "    hidden_W -= lr * grad_hidden_W\n",
    "    hidden_b -= lr * grad_hidden_b\n",
    "    output_W -= lr * grad_output_W\n",
    "    output_b -= lr * grad_output_b\n",
    "    print_dim(\"hidden_W\", hidden_W)\n",
    "    print_dim(\"hidden_b\",hidden_b)\n",
    "    print_dim(\"output_W\",output_W)\n",
    "    print_dim(\"output_b\",output_b)\n",
    "\n",
    "    return hidden_W, hidden_b, output_W, output_b,\n",
    "\n",
    "def predict(softmax_output):\n",
    "    y_pred = np.argmax(softmax_output, axis = 1)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input: (2, 10)\n",
      "shape of hidden_W: (10, 5)\n",
      "shape of hidden_b: (5,)\n",
      "shape of output_W: (5, 2)\n",
      "shape of output_b: (2,)\n",
      "shape of y true: (2, 2)\n",
      "\n",
      "array of hidden_W: [[-0.14848118  1.17751219 -1.03127701 -0.0772543   0.52297342]\n",
      " [ 0.92566197 -0.52334616  1.28815022 -0.0834638   0.90277481]\n",
      " [-0.82047439  1.185998    1.31734853  0.75150415 -1.32165343]\n",
      " [-0.04466684  0.51872513  0.95785976  0.65656255  1.01318212]\n",
      " [ 0.60996038  0.98028128  0.02317266 -0.8461365   0.21649808]\n",
      " [-1.67645635  1.4349832  -0.90422134 -0.17637378 -0.65783895]\n",
      " [ 1.48905172 -1.48027333  0.36390227  1.0280659  -0.11066521]\n",
      " [-1.55171384  1.78703477 -2.34232048 -1.39481407  0.13092026]\n",
      " [ 0.54745995  0.66286395 -0.7036196   0.07264728 -0.33473137]\n",
      " [-1.2195222   0.87494506  0.3630566   0.09251444 -0.58696307]]\n",
      "array of output_W: [[ 0.18810901 -0.18789522]\n",
      " [ 1.06556972 -0.17555015]\n",
      " [ 0.12755784 -0.96823612]\n",
      " [-0.44129954  0.66660278]\n",
      " [-0.29150912  0.80823897]]\n"
     ]
    }
   ],
   "source": [
    "#initialize dataset\n",
    "count_data = 2\n",
    "n_data = 10 # length of each data\n",
    "n_hidden = 5 # width of hidden layer\n",
    "n_output = 2 # width of output layer\n",
    "\n",
    "# initialize each layer\n",
    "input = np.ones((count_data,n_data))\n",
    "hidden_W = np.random.randn(n_data,n_hidden)\n",
    "hidden_b = np.zeros(n_hidden,)\n",
    "output_W = np.random.randn(n_hidden,n_output)\n",
    "output_b = np.zeros(n_output)\n",
    "\n",
    "y_true = np.array([[1,0],[0,1]])\n",
    "\n",
    "print_dim(\"input\",input)\n",
    "print_dim(\"hidden_W\", hidden_W)\n",
    "print_dim(\"hidden_b\",hidden_b)\n",
    "print_dim(\"output_W\",output_W)\n",
    "print_dim(\"output_b\",output_b)\n",
    "print_dim('y true', y)\n",
    "print('')\n",
    "print_arr('hidden_W', hidden_W)\n",
    "print_arr('output_W', output_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of hidden_input: (2, 5)\n",
      "shape of relu_output: (2, 5)\n",
      "shape of softmax_input: (2, 2)\n",
      "shape of softmax_output: (2, 2)\n",
      "\n",
      "array of softmax_input: [[ 7.04245096 -1.14641821]\n",
      " [ 7.04245096 -1.14641821]]\n",
      "array of softmax_output: [[9.99722349e-01 2.77650658e-04]\n",
      " [9.99722349e-01 2.77650658e-04]]\n"
     ]
    }
   ],
   "source": [
    "#run forward\n",
    "softmax_output, relu_output = forward(input,hidden_W, hidden_b, output_W, output_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(softmax_output)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of loss_output: (2, 2)\n",
      "shape of grad_hidden_W: (10, 5)\n",
      "shape of grad_hidden_b: (5,)\n",
      "\n",
      "shape of loss_hidden: (2, 5)\n",
      "shape of grad_output_W: (5, 2)\n",
      "shape of grad_output_b: (2,)\n"
     ]
    }
   ],
   "source": [
    "#implement backward\n",
    "grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b = backward(input,y_true, y_pred, relu_output, hidden_W, hidden_b, output_W, output_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of hidden_W: (10, 5)\n",
      "shape of hidden_b: (5,)\n",
      "shape of output_W: (5, 2)\n",
      "shape of output_b: (2,)\n",
      "\n",
      "array of hidden_W: [[-0.14848118  1.26651414 -1.03127701 -0.05472398  0.52297342]\n",
      " [ 0.92566197 -0.4343442   1.28815022 -0.06093347  0.90277481]\n",
      " [-0.82047439  1.27499996  1.31734853  0.77403448 -1.32165343]\n",
      " [-0.04466684  0.60772709  0.95785976  0.67909288  1.01318212]\n",
      " [ 0.60996038  1.06928323  0.02317266 -0.82360618  0.21649808]\n",
      " [-1.67645635  1.52398516 -0.90422134 -0.15384345 -0.65783895]\n",
      " [ 1.48905172 -1.39127137  0.36390227  1.05059623 -0.11066521]\n",
      " [-1.55171384  1.87603673 -2.34232048 -1.37228375  0.13092026]\n",
      " [ 0.54745995  0.75186591 -0.7036196   0.0951776  -0.33473137]\n",
      " [-1.2195222   0.96394702  0.3630566   0.11504476 -0.58696307]]\n",
      "array of output_W: [[ 0.18810901 -0.18789522]\n",
      " [ 1.72744213  0.48632226]\n",
      " [ 0.12755784 -0.96823612]\n",
      " [-0.43897435  0.66892796]\n",
      " [-0.29150912  0.80823897]]\n"
     ]
    }
   ],
   "source": [
    "#implement update\n",
    "hidden_W, hidden_b, output_W, output = update(0.1, hidden_W, hidden_b, output_W, output_b, grad_hidden_W, grad_hidden_b, grad_output_W, grad_output_b)\n",
    "print('')\n",
    "print_arr('hidden_W', hidden_W)\n",
    "print_arr('output_W', output_W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
