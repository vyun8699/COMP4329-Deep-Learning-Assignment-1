{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fnil\fcharset0 Menlo-Regular;
}
{\colortbl;\red255\green255\blue255;\red140\green211\blue254;\red24\green24\blue24;\red193\green193\blue193;
\red202\green202\blue202;\red212\green214\blue154;\red167\green197\blue152;\red67\green192\blue160;\red140\green211\blue254;
\red24\green24\blue24;\red202\green202\blue202;\red70\green137\blue204;\red194\green126\blue101;}
{\*\expandedcolortbl;;\cssrgb\c61176\c86275\c99608;\cssrgb\c12157\c12157\c12157;\cssrgb\c80000\c80000\c80000;
\cssrgb\c83137\c83137\c83137;\cssrgb\c86275\c86275\c66667;\cssrgb\c70980\c80784\c65882;\cssrgb\c30588\c78824\c69020;\cssrgb\c61176\c86275\c99608;
\cssrgb\c12157\c12157\c12157;\cssrgb\c83137\c83137\c83137;\cssrgb\c33725\c61176\c83922;\cssrgb\c80784\c56863\c47059;}
\paperw11900\paperh16840\margl1440\margr1440\vieww20020\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 COMP5329 - Deep Learning - Assignment 1\
Team : Andrew Zhang: (SID 500543568), Vincent Yunansan (SID 530454874)\
\
\ul A. BACKGROUND:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulnone 1. Goal of this assignment is to implement a neural network without the use of modern machine learning libraries\
2. Teams are provided with training and test data, each with 50,000 and 10,000 data points. Each data point consists of 128 floating numbers. \ul \
\ulnone 3. Each data point has a corresponding label stored in the training and testing label. There are 10 labels to which the data points are assigned to.\ul \ulc0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulc0 \
B. OVERVIEW:\ulnone \
1. Please refer to 
\f1\b assgmt1_DL.ipynb
\f0\b0  for our neural network implementation\
2. The structure of our neural network implementation can be divided into three main parts, explained below:\
	\
\ul 2A. PRE-PROCESSING:\ulnone \
1. 
\f1\b Normalization
\f0\b0 : training and testing data are normalised (mean = c.0, standard deviation = c.1) \
2. 
\f1\b resize_and_shuffle
\f0\b0 : \
	- shuffle: the dataset provided was sorted by class, shuffling the data is important especially if we are using mini-batch\
	- resize: we implemented the capability to downsize the dataset, so we do not have to run our training and testing with the full dataset during the building phase.\
	\
	>>> Sample command: \
	>>> 
\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 data\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 label\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf6 \cb3 \strokec6 resize_and_shuffle\cf4 \cb3 \strokec4 (\cf2 \cb3 \strokec2 input_data\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 input_label\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 proportion\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.5\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 random_state\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 42\cf4 \cb3 \strokec4 )\cb1 \

\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 	>>> This command resizes the training data by 0.5 x 50,000 = 25,000 with a set seed for replicability.\
 \
3. 
\f1\b train_val_split
\f0\b0 : splits the data into training and validation sets, especially important were we to implement early stopping.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 	>>> Sample command: \
	>>> 
\f2 \cf6 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 train_data, train_label, val_data, val_label = train_val_split\cf4 \cb3 \strokec4 (\cf2 \cb3 \strokec2 input\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 output\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 proportion\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.8\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 random_state\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf7 \cb3 \strokec7 42\cf4 \cb3 \strokec4 )\cf4 \cb1 \outl0\strokewidth0 \

\f0 \cf0 \kerning1\expnd0\expndtw0 	>>> This command splits the training data to 0.8 x 25,000 = 20,000 in the training data and 0.2 x 25,000 = 5,000 in the validation data with a set seed for replicability.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\ul 2B. THE DEEP LEARNING MODEL:\ulnone \
The deep learning model and its methods are implemented in three classes:\
\
1. 
\f1\b Activation class
\f0\b0 : this object contains our activation functions and their derivatives. This class is called by HiddenLayer when the neural network is constructed.\
	- Activations for hidden layers: tanh, sigmoid, relu, leaky relu\
	- activation for output layer: softmax\
\
2. 
\f1\b HiddenLayer class
\f0\b0 : this object contains all parameters and methods relevant to the hidden layer. In summary, this class contains:\
	- Weights, biases, and their gradients\
	- forward and backward passes\
	- Implementation of momentum, weight decay, dropout, batch normalisation, and Adam.\
\
3. MLP class: this object contains the main constructor and iterator for our neural network. In summary, this class contains:\
	- initial neural network constructor\
	- forward and backward: which will trigger the forward and backward propagations in all layers\
	- criterion cross entropy: which is our objective function\
	- update: which will update weights and gradients after forward and backward propagation\
	- getBatch: which functions as a data loader for mini batch\
	- fit: which will fit our model with the training data, do periodical validation checks, trigger early stopping (if used), \
	- predict: which predicts labels based on the trained neural network\
	- eval: which evaluates model performance during training\
	- train_loop: which implements a single forward, backward, and update loop, to be used in fit.\
\
	>>> sample command:\
	>>> 
\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 nn\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf8 \cb3 \strokec8 MLP\cf4 \cb3 \strokec4 (\cf2 \cb3 \strokec2 [128,64,64,10]\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 [None, \'91relu\'92,\'92relu\'92,\'92softmax\'92]\cf4 \cb3 \strokec4 )\cb1 \

\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 	>>> This constructs a neural network object with two hidden layers (relu, 64 nodes each) and one output layer (softmax, 10 nodes). \
	\
	>>> 
\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 train_loss\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 val_loss\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 epoch\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 nn\cf4 \cb3 \strokec4 .\cf6 \cb3 \strokec6 fit\cf4 \cb3 \strokec4 (\cf2 \cb3 \strokec2 train_data\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 train_label\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 val_data\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 val_label\cf4 \cb3 \strokec4 , \cb1 \
\pard\pardeftab720\partightenfactor0
\cf4 \cb3                                                             \cf2 \cb3 \strokec2 momentum_gamma\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.0\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 learning_rate\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.001\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 epochs\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 200\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 batch_size\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 100\cf4 \cb3 \strokec4 , \cb1 \
\cb3                                                             \cf2 \cb3 \strokec2 weight_decay\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.0\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 dropout_rate\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.0\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 early_stopping\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 [3,10]\cf4 \cb3 \strokec4 ,  \cb1 \
\cb3                                                             \cf9 \cb10 \outl0\strokewidth0 batchnorm_switch\cf4  \cf11 =\cf4  \cf9 False\cf4 , \cf2 \cb3 \outl0\strokewidth0 \strokec2 adam_switch\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 False\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 adam_learning_rate\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf2 \cb3 \strokec2 0.0\cf4 \cb3 \strokec4 )\cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 	>>> This implements a fit with training and validation data and labels, with different parameters (e.g. learning_rate, epochs, batch_size) and methods.\
	>>> Further explanation is provided in comments within the code file\
\
\ul 2C. HELPER FUNCTIONS:\ulnone \
1. 
\f1\b Hyperparameter_testing
\f0\b0 : \
	- a hyperparameter tuning function is implemented to simplify the repeated hyperparameter testing\
	- this function takes all possible combination of parameters as lists and iterates through them\
\
	>>> sample command:\
	>>> 	
\f2 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 learning_rates\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 0.001\cf4 \cb3 \strokec4 ]\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 		batch_sizes\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 1\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		early_stopping_combination\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [[\cf8 \cb3 \strokec8 np\cf4 \cb3 \strokec4 .inf,\cf7 \cb3 \strokec7 10\cf4 \cb3 \strokec4 ],[\cf7 \cb3 \strokec7 3\cf4 \cb3 \strokec4 ,\cf7 \cb3 \strokec7 10\cf4 \cb3 \strokec4 ]]\cb1 \
\cf2 \cb3 \strokec2 		epoch_counts\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 200\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		momentum_gammas\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 0.0\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		weight_decays\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 0.0\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		dropouts\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 0.0\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		batchnorm_switches\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf12 \cb3 \strokec12 False\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		adam_learning_rates\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [\cf7 \cb3 \strokec7 0.00\cf4 \cb3 \strokec4 ]\cb1 \
\cf2 \cb3 \strokec2 		node_counts\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [[\cf7 \cb3 \strokec7 128\cf4 \cb3 \strokec4 ,\cf7 \cb3 \strokec7 64\cf4 \cb3 \strokec4 ,\cf7 \cb3 \strokec7 64\cf4 \cb3 \strokec4 ,\cf7 \cb3 \strokec7 10\cf4 \cb3 \strokec4 ]] \cb1 \
\cf2 \cb3 \strokec2 		node_activations\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  [[\cf12 \cb3 \strokec12 None\cf4 \cb3 \strokec4 ,\cf13 \cb3 \strokec13 'relu'\cf4 \cb3 \strokec4 ,\cf13 \cb3 \strokec13 'relu'\cf4 \cb3 \strokec4 ,\cf13 \cb3 \strokec13 'softmax'\cf4 \cb3 \strokec4 ]]\cb1 \
\
\cf2 \cb3 \strokec2 		parameters\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  (\cf2 \cb3 \strokec2 learning_rates\cf4 \cb3 \strokec4 ,\cf2 \cb3 \strokec2 batch_sizes\cf4 \cb3 \strokec4 ,\cf2 \cb3 \strokec2 early_stopping_combination\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 epoch_counts\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 momentum_gammas\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 weight_decays\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 dropouts\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 batchnorm_switches\cf4 \cb3 \strokec4 ,\cf2 \cb3 \strokec2 adam_learning_rates\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 node_counts\cf4 \cb3 \strokec4 ,\cf2 \cb3 \strokec2 node_activations\cf4 \cb3 \strokec4 )\cb1 \
\
\cf2 \cb3 \strokec2 		results\cf4 \cb3 \strokec4  \cf5 \cb3 \strokec5 =\cf4 \cb3 \strokec4  \cf6 \cb3 \strokec6 hyperparamater_testing\cf4 \cb3 \strokec4 (\cf2 \cb3 \strokec2 parameters\cf4 \cb3 \strokec4 ,\cf2 \cb3 \strokec2 train_data\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 train_label\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 val_data\cf4 \cb3 \strokec4 , \cf2 \cb3 \strokec2 val_label\cf4 \cb3 \strokec4 )\cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 		>>> This example does hyperparameter tuning over early_stopping combination \
		>>> e.g. ([np.inf,10] for no early stopping, [3,10] for early stopping with 3 max cycle of deteriorating validation score and validation checks every 10 epochs).\
		>>> this function returns relevant parameters and scores for reporting purposes and train and validation losses for plotting purposes.\
		>>> Further explanation is provided in comments within the code file\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
2. 
\f1\b filter_asset
\f0\b0 :\
	- this function creates a data frame from hyperparameter tuning results, for reporting purposes\
	- users can choose how to filter the dataset (e.g. only include results where learning_rate = 0.001) and which columns to show in the data frame.\
\
3. 
\f1\b Plot functions
\f0\b0 :\
	- several plot functions are provided for ease of repeated plotting.\
	- the plot functions allow for customisable colours, lines, and markers.\
\
\

\f1\b \ul C. REFERENCES:
\f0\b0 \ulnone \
\
Preparation of this code has been aided by academic papers, open repositories, and other public online materials for good practices and insights.\
Several notable sources are listed below:\

\f2 \cf13 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec13 1. Yong, Hongwei & Huang, Jianqiang & Meng, Deyu & Hua, Xiansheng & Zhang, Lei. (2020). Momentum Batch Normalization for Deep Learning with Small Batch Size. 10.1007/978-3-030-58610-2_14. \cf4 \cb1 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf13 \cb3 \strokec13 2. Vinod, R. (2021, December 14). Batch Normalisation Explained - Towards Data Science. Medium. https://towardsdatascience.com/batch-normalisation-explained-5f4bd9de5feb#:~:text=Test%20phase,that%20is%20calculated%20during%20training.\cf4 \cb1 \strokec4 \
\cf13 \cb3 \strokec13 3. Kratzert, F. (2016, February 12). Understanding the backward pass through Batch Normalization Layer. https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html
\f0 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\pardeftab720\partightenfactor0

\f2 \cf13 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec13 4. D. (2017, August 25). Adam Optimization Algorithm (C2W2L08). YouTube. https://www.youtube.com/watch?v=JXQT_vxqwIs\cf4 \cb1 \strokec4 \
\cf13 \cb3 \strokec13 5. Kingma, D. P., & Ba, J. (2014, December 22). Adam: A Method for Stochastic Optimization. arXiv.org. https://arxiv.org/abs/1412.6980\cf4 \cb1 \strokec4 \
\
}